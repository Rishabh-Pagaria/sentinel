{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e1e254",
   "metadata": {},
   "source": [
    "# Phishing Dataset Exploration\n",
    "\n",
    "This notebook explores the characteristics of our phishing email dataset, including:\n",
    "- Dataset size and composition\n",
    "- Label distribution\n",
    "- Basic text statistics\n",
    "- Preprocessing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4984da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77bbea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Adjust the path as needed\n",
    "data_path = Path(\"../data/raw/phishing_dataset.csv\")  # Update this path\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display first few rows\n",
    "print(\"First few rows of the dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "# Basic dataset info\n",
    "print(\"\\nDataset Info:\")\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b63404",
   "metadata": {},
   "source": [
    "## Dataset Size and Label Distribution\n",
    "\n",
    "Let's analyze the size of our dataset and the distribution of phishing vs benign labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d4e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset size\n",
    "print(f\"Total number of samples: {len(df)}\")\n",
    "print(f\"Number of features: {df.shape[1]}\")\n",
    "\n",
    "# Label distribution\n",
    "label_counts = df['label'].value_counts()\n",
    "print(\"\\nLabel distribution:\")\n",
    "display(label_counts)\n",
    "\n",
    "# Visualize label distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=label_counts.index, y=label_counts.values)\n",
    "plt.title('Distribution of Labels (Phishing vs Benign)')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Calculate percentages\n",
    "label_percentages = (label_counts / len(df) * 100).round(2)\n",
    "print(\"\\nLabel percentages:\")\n",
    "display(label_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bee14fc",
   "metadata": {},
   "source": [
    "## Text Analysis\n",
    "\n",
    "Let's analyze the characteristics of the email text, including length statistics and basic preprocessing metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac15872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add text length statistics\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['subject_length'] = df['subject'].fillna('').str.len()\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"Text length statistics:\")\n",
    "display(df[['text_length', 'subject_length']].describe())\n",
    "\n",
    "# Visualize text length distributions by label\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(x='label', y='text_length', data=df)\n",
    "plt.title('Email Text Length by Label')\n",
    "plt.ylabel('Length (characters)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x='label', y='subject_length', data=df)\n",
    "plt.title('Subject Length by Label')\n",
    "plt.ylabel('Length (characters)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "display(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0126d21",
   "metadata": {},
   "source": [
    "## Preprocessing Analysis\n",
    "\n",
    "Let's analyze how many emails contain HTML, URLs, and other characteristics that need preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bebd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import regex patterns from prep_phish_jsonl\n",
    "import re\n",
    "SCRIPT_RE = re.compile(r\"(?is)<script.*?>.*?</script>\")\n",
    "STYLE_RE = re.compile(r\"(?is)<style.*?>.*?</style>\")\n",
    "TAG_RE = re.compile(r\"(?s)<[^>]+>\")\n",
    "URL_RE = re.compile(r\"https?://[^\\s)>\\]]+\", re.I)\n",
    "\n",
    "# Analysis functions\n",
    "def has_html(text):\n",
    "    return bool(TAG_RE.search(str(text)))\n",
    "\n",
    "def has_scripts(text):\n",
    "    return bool(SCRIPT_RE.search(str(text)))\n",
    "\n",
    "def get_urls(text):\n",
    "    return URL_RE.findall(str(text))\n",
    "\n",
    "# Add preprocessing flags\n",
    "df['has_html'] = df['text'].apply(has_html)\n",
    "df['has_scripts'] = df['text'].apply(has_scripts)\n",
    "df['url_count'] = df['text'].apply(lambda x: len(get_urls(x)))\n",
    "\n",
    "# Display preprocessing statistics\n",
    "print(\"Preprocessing statistics:\")\n",
    "print(f\"Emails containing HTML: {df['has_html'].sum()} ({(df['has_html'].mean()*100):.1f}%)\")\n",
    "print(f\"Emails containing scripts: {df['has_scripts'].sum()} ({(df['has_scripts'].mean()*100):.1f}%)\")\n",
    "print(f\"Emails containing URLs: {(df['url_count'] > 0).sum()} ({((df['url_count'] > 0).mean()*100):.1f}%)\")\n",
    "\n",
    "# Visualize URL distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(data=df, x='url_count', hue='label', multiple=\"stack\", bins=20)\n",
    "plt.title('Distribution of URL Count by Label')\n",
    "plt.xlabel('Number of URLs in Email')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Show correlation with phishing\n",
    "print(\"\\nCorrelation with phishing labels:\")\n",
    "correlations = {\n",
    "    'Has HTML': df['has_html'].corr(df['label'] == 'phish'),\n",
    "    'Has Scripts': df['has_scripts'].corr(df['label'] == 'phish'),\n",
    "    'URL Count': df['url_count'].corr(df['label'] == 'phish')\n",
    "}\n",
    "display(pd.Series(correlations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd39559c",
   "metadata": {},
   "source": [
    "# Phishing Email Dataset Analysis\n",
    "This notebook provides a comprehensive analysis of our phishing detection dataset, including:\n",
    "1. Data distribution and statistics\n",
    "2. Text characteristics analysis\n",
    "3. Common phishing tactics visualization\n",
    "4. Model confidence patterns\n",
    "5. Entity and domain analysis\n",
    "\n",
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a26033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure Jupyter display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69df4d86",
   "metadata": {},
   "source": [
    "## Data Loading and Initial Processing\n",
    "We'll load our dataset from the JSONL files and convert them to pandas DataFrames for analysis. Our data includes:\n",
    "- Training set\n",
    "- Test set\n",
    "- Evaluation set\n",
    "- Adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e57790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load JSONL file into DataFrame with proper structure\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            example = json.loads(line)\n",
    "            \n",
    "            # Extract text and remove 'EMAIL: ' prefix\n",
    "            text = example['input']\n",
    "            if text.startswith('EMAIL: '):\n",
    "                text = text[7:]\n",
    "                \n",
    "            # Extract other fields\n",
    "            record = {\n",
    "                'text': text,\n",
    "                'label': example['output']['label'],\n",
    "                'confidence': example['output'].get('confidence', None),\n",
    "                'tactics': example['output'].get('tactics', []),\n",
    "                'evidence': example['output'].get('evidence', [])\n",
    "            }\n",
    "            data.append(record)\n",
    "            \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load datasets\n",
    "base_path = '../out_jsonl'\n",
    "train_df = load_jsonl(os.path.join(base_path, 'train.jsonl'))\n",
    "test_df = load_jsonl(os.path.join(base_path, 'test.jsonl'))\n",
    "eval_df = load_jsonl(os.path.join(base_path, 'eval.jsonl'))\n",
    "\n",
    "print(\"Dataset sizes:\")\n",
    "print(f\"Training set: {len(train_df):,} examples\")\n",
    "print(f\"Test set: {len(test_df):,} examples\")\n",
    "print(f\"Evaluation set: {len(eval_df):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f74003",
   "metadata": {},
   "source": [
    "## Data Distribution Analysis\n",
    "Let's analyze the distribution of phishing vs benign emails and their characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c2e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze label distribution\n",
    "def plot_label_distribution(df: pd.DataFrame, title: str):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=df, x='label')\n",
    "    plt.title(f'Label Distribution - {title}')\n",
    "    plt.xlabel('Email Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    total = len(df)\n",
    "    for p in plt.gca().patches:\n",
    "        percentage = f'{100 * p.get_height() / total:.1f}%'\n",
    "        plt.annotate(percentage, (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='bottom')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Plot distributions\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(131)\n",
    "plot_label_distribution(train_df, 'Training Set')\n",
    "plt.subplot(132)\n",
    "plot_label_distribution(test_df, 'Test Set')\n",
    "plt.subplot(133)\n",
    "plot_label_distribution(eval_df, 'Evaluation Set')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Print exact numbers\n",
    "print(\"\\nExact Distribution:\")\n",
    "for name, df in [('Training', train_df), ('Test', test_df), ('Eval', eval_df)]:\n",
    "    dist = df['label'].value_counts()\n",
    "    total = len(df)\n",
    "    print(f\"\\n{name} Set:\")\n",
    "    for label, count in dist.items():\n",
    "        print(f\"{label}: {count:,} ({100 * count/total:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c02a90f",
   "metadata": {},
   "source": [
    "## Text Characteristics Analysis\n",
    "Let's analyze various characteristics of the emails such as:\n",
    "1. Text length distribution\n",
    "2. Word count distribution\n",
    "3. Common words and phrases\n",
    "4. Special character usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e5e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add text characteristics\n",
    "for df in [train_df, test_df, eval_df]:\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    df['word_count'] = df['text'].str.split().str.len()\n",
    "    df['avg_word_length'] = df['text'].apply(lambda x: np.mean([len(w) for w in x.split()]))\n",
    "    df['special_char_count'] = df['text'].apply(lambda x: len(re.findall(r'[^a-zA-Z0-9\\s]', x)))\n",
    "\n",
    "# Plot text length distribution by label\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Text length\n",
    "plt.subplot(131)\n",
    "sns.boxplot(data=train_df, x='label', y='text_length')\n",
    "plt.title('Text Length Distribution by Label')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Word count\n",
    "plt.subplot(132)\n",
    "sns.boxplot(data=train_df, x='label', y='word_count')\n",
    "plt.title('Word Count Distribution by Label')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Special characters\n",
    "plt.subplot(133)\n",
    "sns.boxplot(data=train_df, x='label', y='special_char_count')\n",
    "plt.title('Special Character Count by Label')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nText Characteristics Summary (Training Set):\")\n",
    "print(\"\\nBy Label:\")\n",
    "for label in train_df['label'].unique():\n",
    "    subset = train_df[train_df['label'] == label]\n",
    "    print(f\"\\n{label.upper()}:\")\n",
    "    print(f\"Text Length: mean={subset['text_length'].mean():.1f}, median={subset['text_length'].median():.1f}\")\n",
    "    print(f\"Word Count: mean={subset['word_count'].mean():.1f}, median={subset['word_count'].median():.1f}\")\n",
    "    print(f\"Special Chars: mean={subset['special_char_count'].mean():.1f}, median={subset['special_char_count'].median():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2c7921",
   "metadata": {},
   "source": [
    "The visualizations above show the distribution of various text characteristics across different labels in our dataset. These characteristics help us understand the structural differences between texts of different categories:\n",
    "\n",
    "1. **Text Length Distribution**: Shows how the total character count varies across labels, helping identify if certain categories tend to have longer or shorter texts.\n",
    "\n",
    "2. **Word Count Distribution**: Reveals the verbosity patterns across different labels, indicating whether certain categories typically require more or fewer words to express their content.\n",
    "\n",
    "3. **Special Character Usage**: Illustrates the frequency of non-alphanumeric characters in different categories, which might indicate formatting patterns or writing styles specific to certain labels.\n",
    "\n",
    "These patterns can be valuable features for our classification model and help us understand any potential biases in the dataset. They also provide insights into the preprocessing steps we might need to consider, such as length normalization or special character handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7474679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def analyze_vocabulary(texts, top_n=20):\n",
    "    \"\"\"Analyze vocabulary patterns in a list of texts.\"\"\"\n",
    "    # Tokenize and lowercase all words\n",
    "    all_words = [word.lower() for text in texts for word in word_tokenize(text)]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    content_words = [word for word in all_words if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    # Get word frequencies\n",
    "    word_freq = Counter(content_words)\n",
    "    \n",
    "    return word_freq.most_common(top_n)\n",
    "\n",
    "# Analyze vocabulary by label\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for idx, label in enumerate(train_df['label'].unique()):\n",
    "    subset = train_df[train_df['label'] == label]\n",
    "    word_freq = analyze_vocabulary(subset['text'])\n",
    "    \n",
    "    plt.subplot(2, 2, idx + 1)\n",
    "    words, freqs = zip(*word_freq)\n",
    "    plt.bar(range(len(words)), freqs)\n",
    "    plt.xticks(range(len(words)), words, rotation=45, ha='right')\n",
    "    plt.title(f'Top Words in {label.upper()} Category')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Print some vocabulary statistics\n",
    "print(\"\\nVocabulary Statistics by Label:\")\n",
    "for label in train_df['label'].unique():\n",
    "    subset = train_df[train_df['label'] == label]\n",
    "    word_freq = analyze_vocabulary(subset['text'])\n",
    "    unique_words = len(set(word.lower() for text in subset['text'] for word in word_tokenize(text)))\n",
    "    \n",
    "    print(f\"\\n{label.upper()}:\")\n",
    "    print(f\"Unique words: {unique_words}\")\n",
    "    print(\"Top 10 most frequent words:\")\n",
    "    for word, freq in word_freq[:10]:\n",
    "        print(f\"  {word}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c296824",
   "metadata": {},
   "source": [
    "## Vocabulary Analysis\n",
    "\n",
    "This section examines the vocabulary patterns and word usage across different categories in our dataset. The analysis includes:\n",
    "\n",
    "1. **Word Frequency Distribution**: Bar plots showing the most frequent content words (excluding stopwords) for each category. This helps identify:\n",
    "   - Key terms associated with each label\n",
    "   - Common vocabulary patterns\n",
    "   - Potential discriminative features for classification\n",
    "\n",
    "2. **Vocabulary Statistics**:\n",
    "   - Unique word count per category\n",
    "   - Top frequent words and their occurrence counts\n",
    "   - Insights into the lexical diversity of each category\n",
    "\n",
    "The analysis excludes common English stopwords to focus on meaningful content words. Understanding these vocabulary patterns is crucial for:\n",
    "- Feature engineering in our classification model\n",
    "- Identifying category-specific terminology\n",
    "- Understanding the linguistic characteristics of each label"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
