{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1d9fea7",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d49b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.49.0 datasets==3.3.2 accelerate==1.4.0 bitsandbytes==0.45.3 trl==0.15.2 peft==0.14.0 sentencepiece protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7817ca",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0713e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede5e67b",
   "metadata": {},
   "source": [
    "## 3. Import Libraries and Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1baf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Check GPU\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6804e142",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff87f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets from Google Drive\n",
    "print(\"Loading datasets from Google Drive...\")\n",
    "dataset = load_dataset(\"json\", data_files={\n",
    "    \"train\": \"/content/drive/MyDrive/phishing_project/train_gemma.jsonl\",\n",
    "    \"test\": \"/content/drive/MyDrive/phishing_project/eval_gemma.jsonl\"\n",
    "})\n",
    "\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Eval samples: {len(dataset['test'])}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample data:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b578385d",
   "metadata": {},
   "source": [
    "## 5. Convert to Conversational Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cdb0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversation(sample):\n",
    "    \"\"\"\n",
    "    Convert JSONL sample to conversational format for SFTTrainer.\n",
    "    SFTTrainer expects 'messages' format with role-based conversation.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": f\"{sample['instruction']}\\n\\n{sample['input']}\"},\n",
    "            {\"role\": \"assistant\", \"content\": sample['output']}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Convert datasets\n",
    "print(\"Converting to conversational format...\")\n",
    "dataset = dataset.map(create_conversation, remove_columns=list(dataset[\"train\"].features), batched=False)\n",
    "\n",
    "print(\"\\nConverted sample:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6fdb08",
   "metadata": {},
   "source": [
    "## 6. Configure Quantization (QLoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f761cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization config for QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(\"âœ“ QLoRA configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e79137",
   "metadata": {},
   "source": [
    "## 7. Load Tokenizer\n",
    "\n",
    "**Note:** Using instruction-tuned tokenizer which has chat template support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445c9445",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading tokenizer from HuggingFace...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\",  # Instruction-tuned tokenizer with chat template\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Set padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(\"âœ“ Tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19936606",
   "metadata": {},
   "source": [
    "## 8. Load Model\n",
    "\n",
    "**Downloads ~5GB - takes ~30 seconds on Colab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d24251",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Gemma-2-2b model from HuggingFace...\")\n",
    "print(\"This will download ~5GB (takes ~30 seconds on Colab)\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b\",  # Downloads automatically\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\",\n",
    ")\n",
    "\n",
    "print(\"âœ“ Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab72aab",
   "metadata": {},
   "source": [
    "## 9. Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bf1bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration (as per Google's Gemma guide)\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",  # Google's recommendation for Gemma\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "print(\"âœ“ LoRA configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaa37cd",
   "metadata": {},
   "source": [
    "## 10. Configure Training\n",
    "\n",
    "**Training Parameters:**\n",
    "- Epochs: 3\n",
    "- Batch size: 4 (effective 16 with gradient accumulation)\n",
    "- Learning rate: 2e-4\n",
    "- Optimizer: paged_adamw_8bit (memory efficient)\n",
    "- **Results saved to Google Drive** for persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c76b094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"/content/drive/MyDrive/phishing_project/gemma-2-2b-phishing\",  # Save to Drive!\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,  # Can use larger batch on Colab T4\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "print(\"âœ“ Training configuration ready\")\n",
    "print(f\"Output will be saved to: {training_args.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526dfb57",
   "metadata": {},
   "source": [
    "## 11. Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30054e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up SFTTrainer...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Trainer ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108905d9",
   "metadata": {},
   "source": [
    "## 12. Train Model\n",
    "\n",
    "**This will take ~2-3 hours on T4 GPU**\n",
    "\n",
    "**Important:** The model will be saved to Google Drive automatically, so even if Colab disconnects, your progress is safe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc1e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸš€ Starting training...\")\n",
    "print(\"Estimated time: 2-3 hours for 3 epochs\")\n",
    "print(\"Model checkpoints will be saved to Google Drive\")\n",
    "print()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0188f1cb",
   "metadata": {},
   "source": [
    "## 13. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531afa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving final model to Google Drive...\")\n",
    "output_dir = \"/content/drive/MyDrive/phishing_project/gemma-2-2b-phishing-final\"\n",
    "\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"\\nâœ… Model saved to: {output_dir}\")\n",
    "print(\"\\nðŸ“¥ To use locally:\")\n",
    "print(\"1. Download the 'gemma-2-2b-phishing-final' folder from Google Drive\")\n",
    "print(\"2. Extract to 'artifacts/models/gemma-2-2b-phishing' in your local project\")\n",
    "print(\"3. Load with: AutoModelForCausalLM.from_pretrained('artifacts/models/gemma-2-2b-phishing')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944caf4e",
   "metadata": {},
   "source": [
    "## 14. Quick Test (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424d718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the trained model\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=output_dir,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Test sample\n",
    "test_prompt = \"\"\"You are a security model. Classify the following email as 'phishing' or 'safe'. Reply with exactly one word: phishing or safe.\n",
    "\n",
    "Subject: Verify your account now!\n",
    "From: support@paypa1.com\n",
    "Body: Your account has been suspended. Click here to verify immediately.\n",
    "\n",
    "Classification:\"\"\"\n",
    "\n",
    "result = pipe(test_prompt, max_new_tokens=10, temperature=0.1)\n",
    "print(\"\\nTest Result:\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfef974",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Training Complete!**\n",
    "\n",
    "Your fine-tuned Gemma-2-2b model is saved in Google Drive at:\n",
    "- Checkpoints: `/MyDrive/phishing_project/gemma-2-2b-phishing/`\n",
    "- Final model: `/MyDrive/phishing_project/gemma-2-2b-phishing-final/`\n",
    "\n",
    "**Next Steps:**\n",
    "1. Download the model folder from Google Drive\n",
    "2. Use it in your local ensemble with DeBERTa\n",
    "3. Evaluate performance and create ensemble metrics\n",
    "\n",
    "**Benefits over local training:**\n",
    "- âœ… 2-3 hours vs 12-15 hours locally\n",
    "- âœ… Free GPU (T4)\n",
    "- âœ… Persistent storage in Google Drive\n",
    "- âœ… No local environment conflicts"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
